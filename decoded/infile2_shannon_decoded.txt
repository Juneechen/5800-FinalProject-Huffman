Data compression has a history that predates physical computing. Morse code, for example, compresses information by assigning shorter codes to characters that are statistically common in the English language (such as the letters “e” and “t”). Huffman coding came about as the result of a class project at MIT by its then student, David Huffman.

In 1951, Huffman was taking a class under Robert Fano, who (with the help of an engineer and mathematician by the name of Claude Shannon) invented an efficiency scheme known as Shannon-Fano coding. When Fano gave his class the opportunity to either write a term paper or take a final exam, Huffman chose the term paper, which sought to find an efficient binary coding method. This resulted in Huffman coding, which by the 1970s had become a prominent digital encoding algorithm.